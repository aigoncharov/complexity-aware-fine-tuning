{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfde8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mmlu_qwen_train_df_easy = pd.read_csv(\n",
    "    \"../../../data/data_splits/entropy_fallback/qwen/train_df_easy.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    ")\n",
    "mmlu_qwen_train_df_mid = pd.read_csv(\n",
    "    \"../../../data/data_splits/entropy_fallback/qwen/train_df_middle.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    ")\n",
    "mmlu_qwen_train_df_hard = pd.read_csv(\n",
    "    \"../../../data/data_splits/entropy_fallback/qwen/train_df_hard.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    ")\n",
    "\n",
    "mmlu_phi4_train_df_easy = pd.read_csv(\n",
    "    \"../../../data/data_splits/entropy_fallback/phi/train_df_easy.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    ")\n",
    "mmlu_phi4_train_df_mid = pd.read_csv(\n",
    "    \"../../../data/data_splits/entropy_fallback/phi/train_df_middle.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    ")\n",
    "mmlu_phi4_train_df_hard = pd.read_csv(\n",
    "    \"../../../data/data_splits/entropy_fallback/phi/train_df_hard.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb9e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_df = pd.read_csv(\"../../../data/out/distillation/mmlu_deepseek_v3.tsv\", sep=\"\\t\", header=0)\n",
    "distill_df = distill_df[[\"distill_response\", \"question_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624b9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def count_response_tokens(df, tokenizer: AutoTokenizer, response_column: str):\n",
    "    token_cnt = 0\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        response = row[response_column]\n",
    "\n",
    "        if type(response) is not str:\n",
    "            continue\n",
    "\n",
    "        tokens = tokenizer.encode(response)\n",
    "        token_cnt += len(tokens)\n",
    "\n",
    "    return token_cnt\n",
    "\n",
    "\n",
    "def count_response_tokens_by_split(easy_df, mid_df, hard_df, distill_df, tokenizer):\n",
    "    mmlu_df_easy_distill = pd.merge(easy_df, distill_df, on=\"question_id\")\n",
    "    mmlu_df_mid_distill = pd.merge(mid_df, distill_df, on=\"question_id\")\n",
    "    mmlu_df_hard_distill = pd.merge(hard_df, distill_df, on=\"question_id\")\n",
    "\n",
    "    distill_response_column = \"distill_response\"\n",
    "\n",
    "    distill_response_token_cnt_easy = count_response_tokens(mmlu_df_easy_distill, tokenizer, distill_response_column)\n",
    "    distill_response_token_cnt_mid = count_response_tokens(mmlu_df_mid_distill, tokenizer, distill_response_column)\n",
    "    distill_response_token_cnt_hard = count_response_tokens(mmlu_df_hard_distill, tokenizer, distill_response_column)\n",
    "\n",
    "    sft_response_column = \"answer_index\"\n",
    "\n",
    "    easy_df[sft_response_column] = (easy_df[sft_response_column] + 1).astype(str)\n",
    "    mid_df[sft_response_column] = (mid_df[sft_response_column] + 1).astype(str)\n",
    "    hard_df[sft_response_column] = (hard_df[sft_response_column] + 1).astype(str)\n",
    "\n",
    "    sft_response_token_cnt_easy = count_response_tokens(easy_df, tokenizer, sft_response_column)\n",
    "    sft_response_token_cnt_mid = count_response_tokens(mid_df, tokenizer, sft_response_column)\n",
    "    sft_response_token_cnt_hard = count_response_tokens(hard_df, tokenizer, sft_response_column)\n",
    "\n",
    "    total_distill_token_count = (\n",
    "        distill_response_token_cnt_easy + distill_response_token_cnt_mid + distill_response_token_cnt_hard\n",
    "    )\n",
    "    total_sft_token_count = sft_response_token_cnt_easy + sft_response_token_cnt_mid + sft_response_token_cnt_hard\n",
    "    pipeline_token_count = sft_response_token_cnt_easy + sft_response_token_cnt_mid + distill_response_token_cnt_hard\n",
    "    alternative_token_count = (\n",
    "        distill_response_token_cnt_easy + distill_response_token_cnt_mid + sft_response_token_cnt_hard\n",
    "    )\n",
    "\n",
    "    return total_sft_token_count, total_distill_token_count, alternative_token_count, pipeline_token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf8291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 1094.47it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 888.71it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 796.33it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 40892.56it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 39578.03it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 41661.13it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 1215.56it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 1069.60it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 1091.96it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 34713.08it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 38351.62it/s]\n",
      "100%|██████████| 900/900 [00:00<00:00, 39243.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen 3B token count: sft = 2948, distill = 1956671, alternative = 1158232, pipeline = 801387, advantage = 0.5904334453773782\n",
      "Phi4-mini token count: sft = 2700, distill = 1481696, alternative = 950794, pipeline = 533602, advantage = 0.6398707967086366\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "qwen_token_count = count_response_tokens_by_split(\n",
    "    mmlu_qwen_train_df_easy,\n",
    "    mmlu_qwen_train_df_mid,\n",
    "    mmlu_qwen_train_df_hard,\n",
    "    distill_df,\n",
    "    AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\"),\n",
    ")\n",
    "\n",
    "phi4_token_count = count_response_tokens_by_split(\n",
    "    mmlu_phi4_train_df_easy,\n",
    "    mmlu_phi4_train_df_mid,\n",
    "    mmlu_phi4_train_df_hard,\n",
    "    distill_df,\n",
    "    AutoTokenizer.from_pretrained(\"microsoft/Phi-4-mini-instruct\"),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Qwen 3B token count: sft = {qwen_token_count[0]}, distill = {qwen_token_count[1]}, alternative = {qwen_token_count[2]}, pipeline = {qwen_token_count[3]}, advantage = {1 - qwen_token_count[3] / qwen_token_count[1]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Phi4-mini token count: sft = {phi4_token_count[0]}, distill = {phi4_token_count[1]}, alternative = {phi4_token_count[2]}, pipeline = {phi4_token_count[3]}, advantage = {1 - phi4_token_count[3] / phi4_token_count[1]}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
